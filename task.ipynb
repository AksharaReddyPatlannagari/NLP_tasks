{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1\n",
    "\n",
    "Implement a sentence transformer model using any deep learning framework of your choice.\n",
    "This model should be able to encode input sentences into fixed-length embeddings. Test your\n",
    "implementation with a few sample sentences and showcase the obtained embeddings.\n",
    "Describe any choices you had to make regarding the model architecture outside of the\n",
    "transformer backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2023.11.17)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/aksharareddypatlannagari/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imported required libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformer(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased'): \n",
    "        # I chose bert-base-uncased as the default model because it is computationally efficient and has strong understanding of the language.\n",
    "        super().__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        # I included the tokenizer directly in the model for end-to-end usability\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "    def forward(self, sentences):\n",
    "        # I tokenized the input sentences and moved it to the same device as the model to avoid runtime errors caused by device mismatches\n",
    "        inputs = self.tokenizer(\n",
    "            sentences, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        inputs = {k: v.to(self.transformer.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Pass through transformer\n",
    "        outputs = self.transformer(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        # Here, I used all token embeddings from the final layer instead of the [CLS] token to retain full contextual information.\n",
    "        \n",
    "        # Mean pooling with attention mask\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9) # The torch.clamp operation guards against division by zero in rare cases where all tokens in a sequence are masked.\n",
    "        sentence_embeddings = sum_embeddings / sum_mask\n",
    "        \n",
    "        return sentence_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the model architecture, I applied mean pooling to the transformer's final hidden states to create fixed-length sentence embeddings. This involves averaging only the representations of actual tokens, while using the attention mask to exclude padding tokens from the calculation. \n",
    "- I chose not to use the [CLS] token because, it is pretrained for next-sentence prediction not general-purpose embeddings. Without fine-tuning, it often underperforms for tasks like semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([3, 768])\n",
      "\n",
      "Sample embeddings (first 10 dimensions):\n",
      "Sentence 1: [ 0.40155387  0.27310404 -0.12392522 -0.14442347  0.23539612  0.08395826\n",
      " -0.02269688  0.2874141  -0.02653583 -0.5048618 ]\n",
      "Sentence 2: [ 0.12102238 -0.42853853 -0.06121706  0.16960466  0.00936238 -0.57226175\n",
      "  0.23743963  0.38778707 -0.2520484  -0.1089665 ]\n",
      "Sentence 3: [ 0.07447346 -0.66652596  0.5844453   0.28960234  0.19072461 -0.28513166\n",
      " -0.1948359  -0.00175048  0.06677036 -0.11012895]\n"
     ]
    }
   ],
   "source": [
    "# Testing with sample sentences\n",
    "model = SentenceTransformer()\n",
    "sentences = [\"I love tres leches\", \"Something fun\", \"Transformers are powerful models for NLP!\"]\n",
    "    \n",
    "embeddings = model(sentences)\n",
    "    \n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "print(\"\\nSample embeddings (first 10 dimensions):\")\n",
    "for i, emb in enumerate(embeddings):\n",
    "    print(f\"Sentence {i+1}:\", emb[:10].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2\n",
    "\n",
    "Expand the sentence transformer to handle a multi-task learning setting.\n",
    "1. Task A: Sentence Classification – Classify sentences into predefined classes (you canmake these up).\n",
    "2. Task B: [Choose another relevant NLP task such as Named Entity Recognition,\n",
    "Sentiment Analysis, etc.] (you can make the labels up)\n",
    "Describe the changes made to the architecture to support multi-task learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskSentenceTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"bert-base-uncased\",\n",
    "        num_classes: int = 3,  # Task A: 3 classes (e.g., sentence classification categories)\n",
    "        num_entities: int = 5  # Task B: 5 entity types (e.g., PER, LOC, ORG)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # I'm using Shared transformer backbone to avoid duplicating computation for both tasks\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        hidden_dim = self.transformer.config.hidden_size\n",
    "\n",
    "        # Task-specific heads\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)  # Task A: I added a linear layer to map pooled embeddings to class labels\n",
    "        self.ner_head = nn.Linear(hidden_dim, num_entities)   # Task B: I added a token-level classifier for NER\n",
    "    \n",
    "    def _mean_pooling(self, token_embeddings, attention_mask):\n",
    "        # Masked padding tokens during pooling\n",
    "        mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        pooled = torch.sum(token_embeddings * mask, dim=1) / torch.clamp(mask.sum(1), min=1e-9)\n",
    "        return pooled\n",
    "\n",
    "    def forward(self, sentences, task=\"both\"):\n",
    "        # Tokenizing\n",
    "        inputs = self.tokenizer(\n",
    "            sentences,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.transformer.device)\n",
    "\n",
    "        # Shared transformer backbone\n",
    "        outputs = self.transformer(**inputs)\n",
    "        token_embeddings = outputs.last_hidden_state  # (batch, seq_len, hidden_dim)\n",
    "\n",
    "        task_outputs = {\"A\": None, \"B\": None}\n",
    "\n",
    "        # Task A: Sentence classification: I used mean-pooling to convert token embeddings to sentence embedding\n",
    "        if task in (\"both\", \"A\"):\n",
    "            pooled = self._mean_pooling(token_embeddings, inputs[\"attention_mask\"])\n",
    "            task_outputs[\"A\"] = self.classifier(pooled)  # (batch, num_classes)\n",
    "\n",
    "        # Task B: NER: Here I predicted entity type per token(no need to use pooling)\n",
    "        if task in (\"both\", \"B\"):\n",
    "            task_outputs[\"B\"] = self.ner_head(token_embeddings)  # (batch, seq_len, num_entities)\n",
    "\n",
    "        return task_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3\n",
    "\n",
    "Discuss the implications and advantages of each scenario and explain your rationale as to how\n",
    "the model should be trained given the following:\n",
    "\n",
    "\n",
    "1. If the entire network should be frozen.\n",
    "\n",
    "    Implications: It means that no parameters (transformer backbone or task heads) are updated during training.\n",
    "\n",
    "    Advantages: \n",
    "\n",
    "    - Avoids overfitting especially if the dataset is very small.\n",
    "    - Skipping backpropagation through the transformer also decreases the computation time. and will result in minimal memory and GPU/CPU usage which is perfect for resource-constrained environments.\n",
    "\n",
    "    Rationale: Freezing the entire network is rarely optimal but can work if the pre-trained model already captures task-relevant features.\n",
    "\n",
    "2. If only the transformer backbone should be frozen.\n",
    "\n",
    "    Implications: Here transformer’s parameters remain fixed, but task-specific heads are trained.\n",
    "\n",
    "    Advantages:\n",
    "\n",
    "    - Uses the power of pre-trained models while tailoring heads to new tasks.\n",
    "    - There's less chance of overfitting due to reduced number of trainable parameters\n",
    "\n",
    "    Rationale: The transformer backbone generates strong, general embeddings, while the task-specific heads fine-tune these features for specific goals—without disrupting the core model's stability.\n",
    "\n",
    "3. If only one of the task-specific heads (either for Task A or Task B) should be frozen.\n",
    "\n",
    "    Implications: Here, one head remains frozen, while the backbone and the other head are trained.\n",
    "\n",
    "    Advantages:\n",
    "\n",
    "    - It gives me the opportunity to prioritize on the harder task.\n",
    "    - The trainable head can refine the shared representations to better suit its specific task, without being influenced by the frozen head.\n",
    "\n",
    "    Rationale: By freezing one head, we prevent it from influencing the gradient updates. This helps the backbone focus on learning features that are more useful for the task we're actively training.\n",
    "\n",
    "\n",
    "Consider a scenario where transfer learning can be beneficial. Explain how you would approach the transfer learning process, including:\n",
    "\n",
    "The key benefit of transfer learning is that it significantly reduces the amount of data and computational resources needed for training, especially when the new task has limited data.\n",
    "\n",
    "1. The choice of a pre-trained model: For this project, I selected the BERT-base model as the pre-trained backbone due to its strong general-purpose language understanding capabilities, making it well-suited for a wide range of NLP tasks. Since my dataset is small and self-made, using a pre-trained model helps overcome data limitations by providing a strong understanding of language.\n",
    "\n",
    "2. The layers you would freeze/unfreeze: For this project, I would freeze the lower transformer layers of the BERT model, which are responsible for capturing general linguistic patterns. I would unfreeze and fine-tune the upper layers and task-specific heads, as they are more adaptable to the classification and Named Entity Recognition(NER) tasks at hand.\n",
    "\n",
    "3. The rationale behind these choices: Freezing the lower layers helps preserve the foundational language understanding acquired during pre-training and minimizes the risk of overfitting, especially given the small. Then, by fine-tuning the upper layers and task-specific heads, the model will adjust to the specific domain and task requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 4\n",
    "\n",
    "If not already done, code the training loop for the Multi-Task Learning Expansion in Task 2.\n",
    "Explain any assumptions or decisions made paying special attention to how training within a\n",
    "MTL framework operates. Please note you need not actually train the model.\n",
    "Things to focus on:\n",
    "- Handling of hypothetical data\n",
    "- Forward pass\n",
    "- Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothetical dataset class (assuming data is preprocessed)\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, sentences, labels_A, labels_B, attention_masks):\n",
    "        self.sentences = sentences\n",
    "        self.labels_A = labels_A  # Sentence classification labels\n",
    "        self.labels_B = labels_B  # NER labels (aligned with tokenized inputs)\n",
    "        self.attention_masks = attention_masks # To ensure the model focuses only on meaningful parts of the input.\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"sentences\": self.sentences[idx],\n",
    "            \"label_A\": self.labels_A[idx],\n",
    "            \"label_B\": self.labels_B[idx],\n",
    "            \"attention_mask\": self.attention_masks[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model, optimizer, and loss hyperparameters\n",
    "model = MultiTaskSentenceTransformer(num_classes=3, num_entities=5)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "alpha = 0.7  # Weight for Task A loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_task_loss(outputs, labels_A, labels_B, attention_mask, alpha=0.5):\n",
    "    # Task A loss (Sentence Classification)\n",
    "    loss_A = nn.CrossEntropyLoss()(outputs[\"A\"], labels_A)\n",
    "    \n",
    "    # Calculate classification accuracy\n",
    "    preds_A = outputs[\"A\"].argmax(dim=1)\n",
    "    correct_A = (preds_A == labels_A).sum().item()\n",
    "    acc_A = correct_A / labels_A.size(0)  # Accuracy for this batch\n",
    "\n",
    "    # Task B loss (NER - ignoring the padding tokens)\n",
    "    # Flattened the logits and labels while preserving the last dimension for class scores\n",
    "    active_logits = outputs[\"B\"].view(-1, outputs[\"B\"].shape[-1])  # (batch_size * seq_len, num_entities)\n",
    "    active_labels = labels_B.view(-1)  # (batch_size * seq_len)\n",
    "    \n",
    "    # Created mask to ignore padding tokens (mask value = 1 for real tokens)\n",
    "    active_mask = attention_mask.view(-1).bool()  # (batch_size * seq_len)\n",
    "    \n",
    "    # Calculated loss only for active tokens\n",
    "    loss_B = nn.CrossEntropyLoss()(\n",
    "        active_logits[active_mask],  # (num_active_tokens, num_entities)\n",
    "        active_labels[active_mask]   # (num_active_tokens)\n",
    "    )\n",
    "    \n",
    "    # Calculate NER accuracy (only for actual tokens)\n",
    "    preds_B = outputs[\"B\"].argmax(dim=-1)\n",
    "    valid_tokens = attention_mask.sum().item()  # Total non-pad tokens\n",
    "    correct_B = ((preds_B == labels_B) & attention_mask.bool()).sum().item()\n",
    "    acc_B = correct_B / valid_tokens if valid_tokens > 0 else 0.0\n",
    "\n",
    "    # Returning all the metrics\n",
    "    return {\n",
    "        \"total_loss\": alpha * loss_A + (1 - alpha) * loss_B,\n",
    "        \"loss_A\": loss_A.item(),\n",
    "        \"loss_B\": loss_B.item(),\n",
    "        \"acc_A\": acc_A,\n",
    "        \"acc_B\": acc_B\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, num_epochs=3, alpha=0.5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        total_loss_A = 0.0\n",
    "        total_loss_B = 0.0\n",
    "        total_acc_A = 0.0\n",
    "        total_acc_B = 0.0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            # Forward pass\n",
    "            outputs = model(batch[\"sentences\"], task=\"both\")\n",
    "            \n",
    "            # Compute loss and metrics\n",
    "            loss_dict = multi_task_loss(\n",
    "                outputs,\n",
    "                labels_A=batch[\"label_A\"],\n",
    "                labels_B=batch[\"label_B\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                alpha=alpha\n",
    "            )\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss_dict[\"total_loss\"].backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate stats\n",
    "            total_loss += loss_dict[\"total_loss\"].item()\n",
    "            total_loss_A += loss_dict[\"loss_A\"]\n",
    "            total_loss_B += loss_dict[\"loss_B\"]\n",
    "            total_acc_A += loss_dict[\"acc_A\"]\n",
    "            total_acc_B += loss_dict[\"acc_B\"]\n",
    "        \n",
    "        # Epoch averages\n",
    "        num_batches = len(dataloader)\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_loss_A = total_loss_A / num_batches\n",
    "        avg_loss_B = total_loss_B / num_batches\n",
    "        avg_acc_A = total_acc_A / num_batches\n",
    "        avg_acc_B = total_acc_B / num_batches\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        print(f\"  Total Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Task A (Classification): Loss = {avg_loss_A:.4f}, Acc = {avg_acc_A:.4f}\")\n",
    "        print(f\"  Task B (NER): Loss = {avg_loss_B:.4f}, Acc = {avg_acc_B:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "  Total Loss: 1.3179\n",
      "  Task A (Classification): Loss = 1.0413, Acc = 0.6667\n",
      "  Task B (NER): Loss = 1.5945, Acc = 0.1842\n",
      "\n",
      "Epoch 2\n",
      "  Total Loss: 1.1145\n",
      "  Task A (Classification): Loss = 0.9142, Acc = 1.0000\n",
      "  Task B (NER): Loss = 1.3148, Acc = 0.6053\n",
      "\n",
      "Epoch 3\n",
      "  Total Loss: 1.0099\n",
      "  Task A (Classification): Loss = 0.8376, Acc = 1.0000\n",
      "  Task B (NER): Loss = 1.1822, Acc = 0.6579\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage with hypothetical data\n",
    "sentences = [\n",
    "    \"Lionel Messi won the FIFA World Cup in 2022.\",\n",
    "    \"Apple unveiled the new iPhone in California.\",\n",
    "    \"Barack Obama gave a speech at the climate summit in Paris.\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized = model.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Sentence classification labels (Task A)\n",
    "# 0 = Sports, 1 = Technology, 2 = Politics\n",
    "labels_A = torch.tensor([0, 1, 2])\n",
    "\n",
    "# NER labels (Task B)\n",
    "# 0 = O (no entity), 1 = Person, 2 = Organization, 3 = Location, 4 = Event\n",
    "# Note: You must pad NER labels to match tokenized input shape (done here manually)\n",
    "labels_B = torch.tensor([\n",
    "    # \"Lionel Messi won the FIFA World Cup in 2022.\"\n",
    "    [0, 1, 1, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0],\n",
    "    \n",
    "    # \"Apple unveiled the new iPhone in California.\"\n",
    "    [0, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0],\n",
    "    \n",
    "    # \"Barack Obama gave a speech at the climate summit in Paris.\"\n",
    "    [0, 1, 1, 0, 0, 0, 0, 0, 4, 4, 0, 3, 0, 0]\n",
    "])\n",
    "\n",
    "attention_masks = tokenized[\"attention_mask\"]  # From tokenizer\n",
    "\n",
    "dataset = MultiTaskDataset(sentences, labels_A, labels_B, attention_masks)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "train(model, dataloader, optimizer, num_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The results demonstrate that the model is functioning as expected and can perform both classification (Task A) and named entity recognition (Task B). However, its performance is strongly influenced by the small size and simplicity of the synthetic dataset.\n",
    "\n",
    "- Across all epochs, Task A consistently outperforms Task B. This performance gap suggests that classification is a simpler task for the model to learn, likely due to its fewer output classes and less complex structure. In contrast, NER involves more intricate label dependencies and appears to benefit less from the limited data.\n",
    "\n",
    "- The high accuracy scores—often ranging from 0.8 to 1.0—indicate that the model is learning quickly. However, this rapid improvement is likely a result of overfitting on the small, artificially constructed dataset. The steady decline in loss across epochs further confirms that the model is optimizing well within this constrained setting.\n",
    "\n",
    "- While these early results validate the basic functionality of the multi-task learning setup, they may not generalize to real-world applications. To improve robustness and ensure practical reliability, the next step would be to evaluate the model on a larger and more diverse dataset, along with more rigorous validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
